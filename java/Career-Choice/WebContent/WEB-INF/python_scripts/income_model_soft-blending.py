import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import TimeSeriesSplit, GridSearchCV
from xgboost import XGBRegressor
from catboost import CatBoostRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import os

# ==============================================================================
# 1. ë°ì´í„° ë° í”¼ì²˜ ì¤€ë¹„ (Data & Feature Preparation)
# ==============================================================================
def load_data(file_path="klips_data_23.csv"):
    """CSV íŒŒì¼ì—ì„œ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤."""
    return pd.read_csv(file_path)

def prepare_features_and_target(df):
    """ë°ì´í„°í”„ë ˆì„ì—ì„œ í”¼ì²˜ì™€ íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    features = [
        "age", "gender", "education", "monthly_income", "job_category",
        "satis_wage", "satis_stability", "satis_task_content", "satis_work_env",
        "satis_work_time", "satis_growth", "satis_communication",
        "satis_fair_eval", "satis_welfare", "job_satisfaction", "prev_job_satisfaction", 
        "prev_monthly_income", "job_category_income_avg",
        "income_relative_to_job", "job_category_education_avg", "education_relative_to_job",
        "job_category_satisfaction_avg", "age_x_job_category", "monthly_income_x_job_category",
        "education_x_job_category", "income_relative_to_job_x_job_category", "income_diff"
    ]
    X = df[features]
    y = df["income_change_rate"]
    return X, y

def split_data_by_year(df, X, y):
    """ìµœì‹  ì—°ë„ë¥¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì‚¬ìš©í•˜ì—¬ ì‹œê³„ì—´ ë¶„í• ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    latest_year = df["year"].max()
    X_train = X[df["year"] < latest_year]
    y_train = y[df["year"] < latest_year]
    X_test = X[df["year"] == latest_year]
    y_test = y[df["year"] == latest_year]
    return X_train, y_train, X_test, y_test

# ==============================================================================
# 2. ëª¨ë¸ íŠœë‹ ë° í•™ìŠµ (Model Tuning & Training)
# ==============================================================================
def tune_xgboost(X_train, y_train):
    """GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ XGBoost ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•©ë‹ˆë‹¤."""
    tscv = TimeSeriesSplit(n_splits=5)
    xgb_grid = {
        'n_estimators': [100, 200],
        'max_depth': [3, 5],
        'learning_rate': [0.05, 0.1],
        'subsample': [0.8, 1.0]
    }
    xgb_search = GridSearchCV(
        estimator=XGBRegressor(random_state=42),
        param_grid=xgb_grid,
        cv=tscv,
        scoring='neg_root_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    xgb_search.fit(X_train, y_train)
    return xgb_search.best_estimator_

def tune_catboost(X_train, y_train):
    """GridSearchCVë¥¼ ì‚¬ìš©í•˜ì—¬ CatBoost ëª¨ë¸ì˜ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ íŠœë‹í•©ë‹ˆë‹¤."""
    tscv = TimeSeriesSplit(n_splits=5)
    cat_grid = {
        'iterations': [100, 150],
        'depth': [3, 5],
        'learning_rate': [0.05, 0.1]
    }
    cat_search = GridSearchCV(
        estimator=CatBoostRegressor(verbose=0, random_state=42),
        param_grid=cat_grid,
        cv=tscv,
        scoring='neg_root_mean_squared_error',
        n_jobs=-1,
        verbose=1
    )
    cat_search.fit(X_train, y_train, cat_features=["gender", "education", "job_category"])
    return cat_search.best_estimator_

# ==============================================================================
# 3. í‰ê°€ ë° ì‹œê°í™” (Evaluation & Visualization)
# ==============================================================================
def evaluate(name, y_true, y_pred):
    """ëª¨ë¸ ì„±ëŠ¥ì„ í‰ê°€í•˜ê³  ê²°ê³¼ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
    rmse = mean_squared_error(y_true, y_pred, squared=False)
    mae = mean_absolute_error(y_true, y_pred)
    r2 = r2_score(y_true, y_pred)
    print(f"\n-------- {name} --------")
    print(f"ğŸ“‰ RMSE: {rmse:.4f}")
    print(f"ğŸ“‰ MAE : {mae:.4f}")
    print(f"ğŸ“ˆ RÂ²  : {r2:.4f}")

def plot_feature_importance(model, feature_names, model_name, top_n=15):
    """ëª¨ë¸ì˜ í”¼ì²˜ ì¤‘ìš”ë„ë¥¼ ì‹œê°í™”í•©ë‹ˆë‹¤."""
    if isinstance(model, XGBRegressor):
        importances = model.feature_importances_
        palette = 'Greens_d'
    elif isinstance(model, CatBoostRegressor):
        importances = model.get_feature_importance()
        palette = 'Blues_d'
    else:
        return

    fi = pd.DataFrame({
        'Feature': feature_names,
        'Importance': importances
    }).sort_values(by='Importance', ascending=False).head(top_n)
    
    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=fi, palette=palette)
    plt.title(f'{model_name} Feature Importance (Top {top_n})')
    plt.tight_layout()
    plt.show()

# ==============================================================================
# 4. ëª¨ë¸ ì €ì¥ (Model Saving)
# ==============================================================================
def save_models(xgb_model, cat_model):
    """í•™ìŠµëœ ëª¨ë¸ì„ íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    output_dir = "saved_models"
    os.makedirs(output_dir, exist_ok=True)

    xgb_path = os.path.join(output_dir, "xgb_income_change_model.pkl")
    joblib.dump(xgb_model, xgb_path)
    print(f"[ì €ì¥ ì™„ë£Œ] XGBoost ëª¨ë¸ â†’ {xgb_path}")

    cat_path = os.path.join(output_dir, "cat_income_change_model.cbm")
    cat_model.save_model(cat_path)
    print(f"[ì €ì¥ ì™„ë£Œ] CatBoost ëª¨ë¸ â†’ {cat_path}")

# ==============================================================================
# 5. ë©”ì¸ ì‹¤í–‰ ë¡œì§ (Main Execution)
# ==============================================================================
def main():
    """ì „ì²´ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
    # ë°ì´í„° ë¡œë“œ ë° ì¤€ë¹„
    df = load_data()
    X, y = prepare_features_and_target(df)
    X_train, y_train, X_test, y_test = split_data_by_year(df, X, y)

    # ëª¨ë¸ íŠœë‹ ë° í•™ìŠµ
    print("--- XGBoost ëª¨ë¸ íŠœë‹ ì‹œì‘ ---")
    xgb_best = tune_xgboost(X_train, y_train)
    
    print("\n--- CatBoost ëª¨ë¸ íŠœë‹ ì‹œì‘ ---")
    cat_best = tune_catboost(X_train, y_train)

    # ì˜ˆì¸¡
    y_pred_xgb = xgb_best.predict(X_test)
    y_pred_cat = cat_best.predict(X_test)
    
    # Soft-Blending
    alpha = 0.3
    y_pred_blend = alpha * y_pred_xgb + (1 - alpha) * y_pred_cat

    # í‰ê°€
    evaluate("XGBoost (Tuned)", y_test, y_pred_xgb)
    evaluate("CatBoost (Tuned)", y_test, y_pred_cat)
    evaluate("Soft-Blended Ensemble", y_test, y_pred_blend)

    # ì‹œê°í™”
    plot_feature_importance(xgb_best, X.columns, "XGBoost")
    plot_feature_importance(cat_best, X.columns, "CatBoost")

    # ëª¨ë¸ ì €ì¥
    save_models(xgb_best, cat_best)

if __name__ == "__main__":
    main()
